# Mimir MCP Server Environment Configuration
# Copy this file to .env and customize as needed

# ============================================================================
# LLM Configuration (Chat Completions & MCP Tool Execution)
# ============================================================================

# LLM Provider: 'openai'/'copilot' (OpenAI-compatible endpoint) or 'ollama'/'llama.cpp' (local LLM)
# 
# Aliases:
#   - openai, copilot: OpenAI-compatible endpoint (GitHub Copilot or OpenAI API)
#   - ollama, llama.cpp: Local LLM provider (Ollama or llama.cpp, interchangeable)
# 
# Examples:
#   - openai: Use copilot-api proxy for GitHub Copilot (via OpenAI-compatible endpoint)
#   - copilot: Same as openai (GitHub Copilot license)
#   - openai: Use actual OpenAI API endpoint
#   - ollama: Use local Ollama instance
#   - llama.cpp: Use local llama.cpp instance (same as ollama)
LLM_PROVIDER=openai

# LLM API Endpoint URL
# - copilot-api (GitHub Copilot via proxy): http://copilot-api:4141/v1
# - copilot-api (local): http://localhost:4141/v1
# - Ollama (Docker): http://ollama:11434
# - Ollama (local): http://localhost:11434
# - OpenAI: https://api.openai.com/v1
LLM_API_URL=http://copilot-api:4141/v1

# Default chat model
# - For copilot-api/OpenAI: gpt-4o, gpt-4-turbo, gpt-4.1, etc.
# - For Ollama: qwen2.5-coder, deepseek-coder, llama3.1, etc.
DEFAULT_MODEL=gpt-4.1

# Embedding model (used for semantic search and RAG)
# - For OpenAI-compatible: nomic-embed-text (recommended, lightweight)
# - For Ollama: nomic-embed-text, mxbai-embed-large, all-minilm
EMBEDDING_MODEL=nomic-embed-text

# ============================================================================
# Database Configuration
# ============================================================================
NEO4J_PASSWORD=password
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j

# ============================================================================
# Docker Configuration
# ============================================================================

# Host workspace directory to mount in container
# Default: ~/src (your home directory's src folder)
HOST_WORKSPACE_ROOT=~/src

# ============================================================================
# Server Configuration
# ============================================================================

# Node environment (development | production)
NODE_ENV=production

# HTTP server port
PORT=3000

# File watching configuration
FILE_WATCH_POLLING=true
FILE_WATCH_INTERVAL=1000
WORKSPACE_ROOT=/workspace

# ============================================================================
# Feature Flags
# ============================================================================

# Enable semantic search with vector embeddings
# Requires LLM endpoint with embeddings support
MIMIR_FEATURE_VECTOR_EMBEDDINGS=true

# Enable PM agent to suggest alternative models based on task requirements
MIMIR_FEATURE_PM_MODEL_SUGGESTIONS=false

# ============================================================================
# Advanced: MCP Memory Configuration
# ============================================================================

MCP_MEMORY_STORE_PATH=/app/data/.mcp-memory-store.json
MCP_MEMORY_SAVE_INTERVAL=10
MCP_MEMORY_TODO_TTL=86400000
MCP_MEMORY_PHASE_TTL=604800000
MCP_MEMORY_PROJECT_TTL=-1