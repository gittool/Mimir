# NornicDB Docker Image with NVIDIA CUDA Support
# Neo4j-compatible graph database with GPU-accelerated local embeddings
#
# PREREQUISITES: Build the llama-cuda-libs image once (takes ~10-15 min):
#   docker build -f Dockerfile.llama-cuda -t timothyswt/llama-cuda-libs:b4785 .
#   docker push timothyswt/llama-cuda-libs:b4785  # optional: share across machines
#
# Then this build is fast (~1-2 min) because CUDA compilation is cached!

# Global ARG must be declared before FROM to use in FROM instruction
ARG LLAMA_CUDA_IMAGE=timothyswt/llama-cuda-libs:b4785

# Stage 1: Build the UI
FROM node:20-alpine AS ui-builder

WORKDIR /ui

# Ensure clean npm config (no auth issues from host)
RUN npm config set registry https://registry.npmjs.org/

# Copy UI package files
COPY ui/package.json ui/package-lock.json* ./

# Install dependencies
RUN npm ci 2>/dev/null || npm install --legacy-peer-deps

# Copy UI source
COPY ui/ .

# Build the UI
RUN npm run build

# Stage 2: Pre-built llama.cpp CUDA libraries (build once, reuse forever)
# Build with: docker build -f Dockerfile.llama-cuda -t timothyswt/llama-cuda-libs:b4785 .
FROM ${LLAMA_CUDA_IMAGE} AS llama-builder

# Stage 3: Build the Go binary with CUDA support
FROM nvidia/cuda:12.6.3-devel-ubuntu22.04 AS builder

# Install Go and OpenMP
ENV GO_VERSION=1.23.4
RUN apt-get update && apt-get install -y wget git gcc g++ libgomp1 && \
    wget https://go.dev/dl/go${GO_VERSION}.linux-amd64.tar.gz && \
    tar -C /usr/local -xzf go${GO_VERSION}.linux-amd64.tar.gz && \
    rm go${GO_VERSION}.linux-amd64.tar.gz
ENV PATH="/usr/local/go/bin:${PATH}"
ENV CUDA_HOME=/usr/local/cuda

WORKDIR /build

# Copy llama.cpp artifacts from pre-built image
COPY --from=llama-builder /output/lib/libllama_linux_amd64_cuda.a /build/lib/llama/libllama_linux_amd64_cuda.a
COPY --from=llama-builder /output/include/*.h /build/lib/llama/

# Copy go mod files
COPY go.mod go.sum ./
RUN go mod download

# Copy source code
COPY . .

# Copy built UI from ui-builder stage
COPY --from=ui-builder /ui/dist ./ui/dist

# Build with CGO enabled for CUDA support and local LLM embeddings, include build timestamp
RUN CGO_ENABLED=1 GOOS=linux go build -tags "cuda localllm" -ldflags="-s -w -X main.buildTime=$(date -u +%Y%m%d-%H%M%S)" -o nornicdb ./cmd/nornicdb

# Runtime stage - use CUDA runtime image (smaller than devel)
FROM nvidia/cuda:12.6.3-runtime-ubuntu22.04

WORKDIR /app

# Install runtime dependencies (libgomp1 required for OpenMP in llama.cpp)
RUN apt-get update && apt-get install -y --no-install-recommends \
    ca-certificates \
    tzdata \
    wget \
    libgomp1 \
    && rm -rf /var/lib/apt/lists/*

# Copy binary from builder
COPY --from=builder /build/nornicdb /app/nornicdb

# Copy entrypoint script
COPY docker-entrypoint.sh /app/docker-entrypoint.sh
RUN chmod +x /app/docker-entrypoint.sh

# Create data directory
RUN mkdir -p /data /data/models

# Expose ports
EXPOSE 7474 7687

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=10s --retries=3 \
  CMD wget --spider -q http://localhost:7474/health || exit 1

# Default environment variables
ENV NORNICDB_DATA_DIR=/data \
    NORNICDB_HTTP_PORT=7474 \
    NORNICDB_BOLT_PORT=7687 \
    NORNICDB_EMBEDDING_PROVIDER=local \
    NORNICDB_EMBEDDING_MODEL=bge-m3 \
    NORNICDB_EMBEDDING_DIMENSIONS=1024 \
    NORNICDB_MODELS_DIR=/data/models \
    NORNICDB_EMBEDDING_GPU_LAYERS=-1 \
    NORNICDB_NO_AUTH=true \
    NORNICDB_GPU_ENABLED=true \
    NVIDIA_VISIBLE_DEVICES=all \
    NVIDIA_DRIVER_CAPABILITIES=compute,utility

ENTRYPOINT ["/app/docker-entrypoint.sh"]
CMD []
